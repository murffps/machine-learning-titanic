{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5AvLmd4Ymjut+K2A0fnxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murffps/machine-learning-titanic/blob/main/MLBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Project Introduction ########\n",
        "Titanic Data Prediction with Classification Supervised Learning : \n",
        "\n",
        "*   Prediction label for an individual as \"survived\" or \"died\"\n",
        "\n",
        "*    take passenger/trip information and predict whether that passenger would survive:  \n",
        "\n",
        "*   Is this a good ML question for good data for ML ? Why ? Why not? initial assumptions.\n",
        "\n"
      ],
      "metadata": {
        "id": "90CLU2dda0dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algebreic Representation of Intent to create a function \n",
        " y = f(x) to turn a feature into a label \n",
        " x is a matrix, every column in x is a feature \n",
        " the output y is a vector that contains labels(or targets) (classification uses labels or regression would use values) \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "QsmwcvVyg4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ML WORKFLOW STEPS \n",
        "Gather\n",
        "clean \n",
        "Create feautres\n",
        "normlaize \n",
        "\n",
        "sample data\n",
        "testing data\n",
        "training data\n",
        "create model \n",
        "evaluate model - possible change question\n",
        "deploy model"
      ],
      "metadata": {
        "id": "zPXKzLacf5Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model that serves as the base for a data science process. It has six sequential phases:\n",
        "\n",
        "Business understanding – What does the business need?\n",
        "Data understanding – What data do we have / need? Is it clean?\n",
        "Data preparation – How do we organize the data for modeling?\n",
        "Modeling – What modeling techniques should we apply?\n",
        "Evaluation – Which model best meets the business objectives?\n",
        "Deployment – How do stakeholders access the results?\n",
        "Published in 1999 to standardize data mining processes across industries, it has since become the most common methodology for data mining, analytics, and data science projects.\n",
        "\n",
        "Data science teams that combine a loose implementation of CRISP-DM with overarching team-based agile project management approaches will likely see the best results.\n",
        "\n",
        "SOURCES:\n",
        "https://www.datascience-pm.com/crisp-dm-2/\n",
        "\n",
        "\n",
        "https://web.archive.org/web/20220401041957/https://www.the-modeling-agency.com/crisp-dm.pdf\n",
        "\n",
        "\n",
        "https://www.oreilly.com/library/view/machine-learning-pocket/9781492047537/\n",
        "Book sections\n",
        "Introduction\n",
        "Overview of the Machine Learning Processing\n",
        "Classification Walkthrough: Titanic Dataset\n",
        "Missing Data\n",
        "Cleaning Data\n",
        "Exploring\n",
        "Preprocess Data\n",
        "Feature Selection\n",
        "Imbalanced Classes\n",
        "Classification\n",
        "Model Selection\n",
        "Metrics and Classification Evaluation\n",
        "Explaining Models\n",
        "Regression\n",
        "Metrics and Regression Evaluation\n",
        "Explaining Regression Models\n",
        "Dimensionality Reduction\n",
        "Clustering\n",
        "Pipelines\n"
      ],
      "metadata": {
        "id": "oQJReZMedTe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create requirements.txt with required python code packages\n",
        "%%writefile requirements.txt\n",
        "matplotlib\n",
        "pandas\n",
        "sklearn \n",
        "yellowbrick\n",
        "xlrd"
      ],
      "metadata": {
        "id": "17olroObDUtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install \n",
        "!pip install -r requirements.txt\n",
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip "
      ],
      "metadata": {
        "id": "qTFYNUwKDXzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tools and libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import (ensemble, preprocessing,tree)\n",
        "from sklearn.metrics import (auc, confusion_matrix, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
        "from yellowbrick.classifier import (ConfusionMatrix, ROCAUC)\n",
        "from yellowbrick.model_selection import (LearningCurve)\n",
        "import ydata_profiling\n",
        "from ydata_profiling import ProfileReport"
      ],
      "metadata": {
        "id": "PoNAT-O_ay0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GATHER DATA - load an excel file\n",
        "url = (\"https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls\")"
      ],
      "metadata": {
        "id": "UA-rVo0YjAUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFo1kYBAnjFw"
      },
      "source": [
        "### PREPARE DATA - create a data frame with column labels (as opposed to numpy array)\n",
        "df = pd.read_excel(url)\n",
        "orig_df = df\n",
        "\n",
        "# profile, inspect some specific data to analyze or spot check data for format we can use\n",
        "# check for type of values like integer or float, NA or missing data, \n",
        "# leaky features (info that are not available for new data yet)\n",
        "# once quality or readiness is determined CLEAN DATA \n",
        "# Strategy to repace or deal with missing or bad data: Standardize, Normalize, Outliers(check min/max), Missing Data(check for NaN)\n",
        "# a. Use S the most common to  \n",
        "# b. drop them -- One possible strategy is to drop columns with no variance or signal or perfect or very high positive or negative correlation\n",
        "# c. use pandas to create dummy columns with all 0s \n",
        "\n",
        "ydata_profiling.ProfileReport(df)\n",
        "df.shape\n",
        "profile = ProfileReport(df, title=\"data set\", html={'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"file.html\")\n",
        "df.describe().iloc[:, :2]\n",
        "# conclusions: age (interpolate values), cabin, boat, body all going to be dropped -LEAKY - \n",
        "df.isnull().sum()\n",
        "df.isnull().mean() * 100\n",
        "\n",
        "df.sex.value_counts(dropna=False)\n",
        "\n",
        "df.embarked.value_counts(dropna=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE FEATURES \n",
        "name = df.name\n",
        "name.head(3)\n",
        "\n",
        "df = df.drop(\n",
        "    columns=[\n",
        "        \"name\",\n",
        "        \"ticket\",\n",
        "        \"home.dest\",\n",
        "        \"boat\",\n",
        "        \"body\",\n",
        "        \"cabin\"\n",
        "    ])\n",
        "\n",
        "df = pd.get_dummies(df)\n",
        "df.columns\n",
        "\n",
        "df = df.drop(columns=\"sex_male\")\n",
        "# OR df = pd.get_dummies(df, drop_first=True)\n",
        "df.columns\n",
        "\n",
        "y = df.survived\n",
        "X = df.drop(columns=\"survived\")\n",
        "\n",
        "## SAMPLE DATA - pull out 30% \n",
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#not working NameError: name 'model_selection' is not defined\n",
        "\n",
        "# Impute missing values for age on training set "
      ],
      "metadata": {
        "id": "Ddzi2p9CkKhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View HTML file \n",
        "import IPython\n",
        "IPython.display.HTML(filename='/content/file.html')"
      ],
      "metadata": {
        "id": "wbTE3WmIKNBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9u6dN7zBuk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}