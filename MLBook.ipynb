{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNnx+XY+NFV/Kao//15QZtk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murffps/machine-learning-titanic/blob/main/MLBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Project Introduction ########\n",
        "\n",
        "MATURITY OR EACH STEP MATURITY \n",
        "COMPLEXITY \n",
        "COST RATIO \n",
        "TIME ESTIMATE \n",
        "\n",
        "Titanic Data Prediction with Classification Supervised Learning : \n",
        "\n",
        "*   Prediction label for an individual as \"survived\" or \"died\"\n",
        "\n",
        "*    take passenger/trip information and predict whether that passenger would survive:  \n",
        "\n",
        "*   Is this a good ML question for good data for ML ? Why ? Why not? initial assumptions.\n",
        "how to properly document this \n",
        "Cost Time trade offs  princples estimate\n",
        "Business value ROI \n",
        "Success how to measure given the above https://github.com/mattharrison/ml_pocket_reference/blob/master/ch03.ipynb \n",
        "\n",
        "many diff sets of steps - pick one \n"
      ],
      "metadata": {
        "id": "90CLU2dda0dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algebreic Representation of Intent to create a function \n",
        " y = f(x) to turn a feature into a label \n",
        " x is a matrix, every column in x is a feature \n",
        " the output y is a vector that contains labels(or targets) (classification uses labels or regression would use values) \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "QsmwcvVyg4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model that serves as the base for a data science process. It has six sequential phases:\n",
        "\n",
        "Business understanding – What does the business need?\n",
        "Data understanding – What data do we have / need? Is it clean?\n",
        "Data preparation – How do we organize the data for modeling?\n",
        "Modeling – What modeling techniques should we apply?\n",
        "Evaluation – Which model best meets the business objectives?\n",
        "Deployment – How do stakeholders access the results?\n",
        "Published in 1999 to standardize data mining processes across industries, it has since become the most common methodology for data mining, analytics, and data science projects.\n",
        "\n",
        "Data science teams that combine a loose implementation of CRISP-DM with overarching team-based agile project management approaches will likely see the best results.\n",
        "\n",
        "SOURCES:\n",
        "https://www.datascience-pm.com/crisp-dm-2/\n",
        "\n",
        "\n",
        "https://web.archive.org/web/20220401041957/https://www.the-modeling-agency.com/crisp-dm.pdf\n",
        "\n",
        "\n",
        "https://www.oreilly.com/library/view/machine-learning-pocket/9781492047537/\n",
        "Book sections\n",
        "Introduction\n",
        "Overview of the Machine Learning Processing\n",
        "Classification Walkthrough: Titanic Dataset\n",
        "Missing Data\n",
        "Cleaning Data\n",
        "Exploring\n",
        "Preprocess Data\n",
        "Feature Selection\n",
        "Imbalanced Classes\n",
        "Classification\n",
        "Model Selection\n",
        "Metrics and Classification Evaluation\n",
        "Explaining Models\n",
        "Regression\n",
        "Metrics and Regression Evaluation\n",
        "Explaining Regression Models\n",
        "Dimensionality Reduction\n",
        "Clustering\n",
        "Pipelines\n",
        "\n",
        "\n",
        "Data set - blip and reason for choice \n"
      ],
      "metadata": {
        "id": "oQJReZMedTe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "- \n",
        "Libs info:\n",
        "pandasai - bascially the same thing but can use BARD and other gpt tools to also write code and ask questions \n",
        "data types : \n",
        "- look for pandas friendly data types to store data--> int64 float64 datetime64[ns] object (combination of string and other). \n",
        "- pandas will try to coerce data and fall back to object, convert date and strings to numeric or feature engineer one. (cardinality h/l and dummy columns?)\n",
        "# KEY information \n",
        "# matplotlib\n",
        "# pandas - as pd - work with data operatios for manipulate, remove columns imort into row collumn tables view tiop section with head()\n",
        "A pandas Series is a one-dimensional array of indexed data. In a Series and most other objects in Python the 0 counts. So, inside most data sets the 0 will actually map to 1.??\n",
        "# sklearn \n",
        "# yellowbrick\n",
        "nltk NLTK is for text mining and natural language processing. not used here \n",
        "\n",
        "numpy NumPy Tools and or techniques including the high-performance multidimensional array object that is a powerful data structure for efficient computation of arrays and matrices.An array is a collection of elements of the same data type. The word axis is singular and axes is plural. AXIS 1 horzontal(along he columns)  axis 0 down the rows \n",
        "Dimensions\n",
        "shape\n",
        "\n",
        "Array Manipulation perations\n",
        "\n",
        "choose over pandas?\n",
        "# # not used yet \n",
        "\n",
        "# Data analysis libs seaborn ydata_profiling\n",
        "# ML statisticvs or math scientific models libs \n",
        "# data frames working with tables and row columns \n",
        "---\n",
        "Statistics Vocabulary  - there is overlap from several industries here. \n",
        "standardizing\n",
        "normalizing \n",
        "\n",
        "Machine Learning or Data Related Definitions \n",
        "leaky features [info that are not available for new data yet or cheating by giving data to the model]\n",
        "\n",
        "Training data is the data used to predict the target variable. \n",
        "That thing or column we are trying to predict is called a target variable. \n"
      ],
      "metadata": {
        "id": "ojOQophuZGAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create requirements.txt with required python code packages\n",
        "%%writefile requirements.txt\n",
        "matplotlib\n",
        "pandas\n",
        "sklearn \n",
        "yellowbrick"
      ],
      "metadata": {
        "id": "17olroObDUtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install \n",
        "!pip install -r requirements.txt\n",
        "!pip install gwpy --quiet #supposed to supress logs\n",
        "# pip install pandasai"
      ],
      "metadata": {
        "id": "qTFYNUwKDXzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tools and libraries\n",
        "import matplotlib.pyplot as plt\n",
        "# When we use the `as` keyword we are creating an alias. It's just a way to reference our library more simply to have less text on the screen for short. \n",
        "import pandas as pd\n",
        "from sklearn import (ensemble, preprocessing,tree)\n",
        "from sklearn.metrics import (auc, confusion_matrix, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
        "from yellowbrick.classifier import (ConfusionMatrix, ROCAUC)\n",
        "from yellowbrick.model_selection import (LearningCurve)\n",
        "# nice way to use profiling in colab notebooks \n",
        "import sys \n",
        "!{sys.executable} -m pip install -U ydata-profiling[notebook]\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "# https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/installation.html\n",
        "from ydata_profiling import ProfileReport\n",
        "import IPython"
      ],
      "metadata": {
        "id": "PoNAT-O_ay0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Machine Learning Process\n",
        "\n",
        "\n",
        "I. Business Understanding\n",
        "TDSP (Team Data Science Process): 2 steps also taken \n",
        "\n",
        "  Data Acquisition & Understanding (Knowledge Information Patterns)\n",
        "\n",
        "  Security \n",
        "  After  evaulatio Customer Acceptance\n",
        "\n",
        "  ML WORKFLOW STEPS \n",
        "Gather\n",
        "clean \n",
        "Create feautres\n",
        "normlaize \n",
        "\n",
        "sample data\n",
        "testing data\n",
        "training data\n",
        "create model \n",
        "evaluate model - possible change question\n",
        "deploy model\n",
        "---\n",
        "Data \n",
        "There are two primary types of numerical data: discrete and continuous data.\n",
        "- continuous refers to data that can be measured.\n",
        "- discrete also referred to as discrete values, is data that only takes certain values. Commonly in the form of whole numbers or integers, this is data that can be counted and has a finite number of values. These values must be able to fall within certain classifications and are unable to be broken down into smaller parts.\n",
        "median vs mode vs mean \n",
        "\n",
        "---\n",
        "Data Survey aka Data Analysis or Exploratory - strategy to determine natrue of data, readiness and quality for use, data sourcesm(strucutre unstructured) help plan for model selection\n",
        "exploratory methods and analytical\n",
        "Understanding data types\n",
        "- seems knowing models help knowing your libraries and how they handle data types \n",
        "possible consult SME can help to know what to do in case of help on nature of your data \n",
        "assumptions, questions \n",
        "time frame estimates avalue proposition on data.. (bad data in bad output) what is possible with this data \n",
        "Profile report \n",
        "extended advacned grapging with seaborn - \n",
        "basically this step is setting up the need and stuff for the steps below\n",
        "estimating and validating ROI for 100 ML model use cases with feedback loop mechanisms post model deployment, than successfully deploying 1 ML model to production.\n",
        "GATHER or GET  DATA\n",
        "(Data Engineering raw data plus future pipelines)\n",
        "\n",
        "\n",
        "Data Preparation and Cleaning (Data Wrangling?) see reverse engineer data wraning tools on aws maybe for additional steps or process define refine\n",
        "\n",
        "PRINCIPLE: \n",
        "APPLICATION:\n",
        "VALUE TIE IN:\n",
        "\n",
        "1. Missing Values - \n",
        "  stratgies - use count for checking for missing data - only includes values not NaN \n",
        "2. Duplicate and Low Varation Data\n",
        "3. Incorrect & Irrelevant Data\n",
        "4. Categorical Data\n",
        "5. Outliers\n",
        "- Leaky data \n",
        "6. Feature Scaling \n",
        "7. Feature Engineering/Selection \n",
        "8. Validation Split "
      ],
      "metadata": {
        "id": "rn1BjAGUDU_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GATHER or GET  DATA - load an excel file \n",
        "url = (\"https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls\")\n",
        "# create a data frame (matrix of data) with column labels \n",
        "# (as opposed to numpy array)\n",
        "df = pd.read_excel(url) \n",
        "orig_df = df "
      ],
      "metadata": {
        "id": "UA-rVo0YjAUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "4bd8a945-4658-4d21-c53e-74726e751b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5256c94a4d1a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create a data frame (matrix of data) with column labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# (as opposed to numpy array)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0morig_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect #rows columns and make initial predictions thoughts like a playbook so to speak \n",
        "df.shape"
      ],
      "metadata": {
        "id": "ys7CE3bGyY-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# snapshot & summary stats\n",
        "df.describe().iloc[:, :2]"
      ],
      "metadata": {
        "id": "r697DM3E3bL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for type of values we mostly want integer or float for now\n",
        "df.dtypes\n",
        "\n",
        "# conclusion, therefor what\n",
        "# why are all these objects? what does that mean? how to deal with?\n",
        "# name          object\n",
        "# sex           object\n",
        "# ticket        object\n",
        "# cabin         object\n",
        "# embarked      object\n",
        "# boat          object\n",
        "# home.dest     object"
      ],
      "metadata": {
        "id": "BMzfB06u3TfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFo1kYBAnjFw"
      },
      "source": [
        "# analyze or spot check data for proper format  \n",
        "df.isnull().sum() # one way to get a count of missing data in each column "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# by default apply axis 0 means along the index - percentage of null values \n",
        "# axis one if you want each sample or along columns \n",
        "df.isnull().sum(axis=1).loc[:10] "
      ],
      "metadata": {
        "id": "0GkeoaDA3rLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the percentage of null values \n",
        "# TIP python treats True and False as 1 and 0 so this shows % of missing data\n",
        "df.isnull().mean() * 100 "
      ],
      "metadata": {
        "id": "zJJ0D_c_3u1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  inspect at rows with missing data by using a Boolean array \n",
        "mask = df.isnull().any(axis=1)\n",
        "mask.head()  # rows\n",
        "# data.columns = data.columns.str.strip() handle white space \n",
        "#conclusions: \n"
      ],
      "metadata": {
        "id": "85w_Lms95HTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[mask].body.head()"
      ],
      "metadata": {
        "id": "PiS01GGt5DM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f. profile data \n",
        "profile = ProfileReport(df, title=\"titanic data set\", html={'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"file.html\")\n",
        "# View HTML profile report\n",
        "IPython.display.HTML(filename='/content/file.html')"
      ],
      "metadata": {
        "id": "gOSUF3c5yWW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# name = df.name\n",
        "# name.head(3)\n",
        "# ? more inspecting priper to drop this one?"
      ],
      "metadata": {
        "id": "6Qw244s6RSSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further inspect data with advanced box plot or histogram to spot outliers wit seaborn (matplotlib ++ or something) \n",
        "#  what needs to meet outside of the profile report ? \n",
        "# Findings from Profile Report ✨ finding issues vs trends suspicions... \n",
        "# ready to move on to cleaning ? when ...\n",
        "#, NA or missing data, outliers, leaky features "
      ],
      "metadata": {
        "id": "6vbDVhS9C0Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the \"spread\" of the data considering:\n",
        "#  variance - the average of the squared differences from the \n",
        "  #mean SYMBOL here\n",
        "# standard deviation  \n",
        "# covariance \n",
        "# correlation\n",
        "\n",
        "# Handle Outliers(check min/max, ) seems like the most common sense while in the graphs plots before the code ones \n",
        "# box and whiskers \n",
        "# https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/learn/lecture/4020118?start=315#overview"
      ],
      "metadata": {
        "id": "SrNNo0yyTTFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN (vs PREPARE? is prepare after import or import a certain way ) STEPS or even OR Data Wrangling \n",
        "# Normalize (\"preprocess\")\n",
        "# Standardize - translate the data so that it has a mean value of 0 and a standard deviation of 1 (helps for scaling)\n"
      ],
      "metadata": {
        "id": "Y3rVBxMBTTyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing data # (check for NaN, null) \n",
        "# Examine - profile gives good intel or we could use missingno\n",
        "\n",
        "# Handle Missing Data unless \n",
        "#: ...XGBoost some like these can handle with missing data ? how what value pro con\n",
        "\n",
        "# age has 263 (20.1%) missing values - strategy to handle  \n",
        "# cabin has 1014 (77.5%) missing values\t- strategy to handle  \n",
        "# boat has 823 (62.9%) missing values\t- strategy to handle  - drop why impute why - S why .. goal - dummy columns \n",
        "# body has 1188 (90.8%) missing values - strategy to handle  \n",
        "# home.dest has 564 (43.1%) missing values - strategy to handle  "
      ],
      "metadata": {
        "id": "-oJ2lqCETQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a. impute -  using alternative values in place of missing data. OR to replace\n",
        "# sibsp has 891 (68.1%) zeros\t 0's is bad data\n",
        "# parch has 1002 (76.5%) zeros\t\n",
        "# fare has 17 (1.3%) zeros\t"
      ],
      "metadata": {
        "id": "aYurXgSMCalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d. use pandas to create dummy columns with all 0s \n",
        "df = pd.get_dummies(df)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "zDiXz-5RQZyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e. drop them -- One possible strategy is to drop columns with \n",
        "     # reasons for dropping: no variance or signal or perfect or very high positive or negative correlation\n",
        "     # # c. Use S the most common to  \n",
        "     \n",
        "# age (interpolate values), \n",
        "# cabin\n",
        "# boat - dtype not supported issues \n",
        "# body missing data in many rows and leaks data (gives survived or deceased)\n",
        "df = df.drop(\n",
        "    columns=[\n",
        "        \"name\",\n",
        "        \"ticket\",\n",
        "        \"home.dest\",\n",
        "        \"boat\",\n",
        "        \"body\",  \n",
        "        \"cabin\" \n",
        "    ]) \n",
        "\n",
        "df.sex.value_counts(dropna=False)\n",
        "df.embarked.value_counts(dropna=False)\n",
        "df = df.drop(columns=\"sex_male\")\n",
        "# OR df = pd.get_dummies(df, drop_first=True)\n",
        "df.columns\n",
        "y = df.survived\n",
        "X = df.drop(columns=\"survived\")"
      ],
      "metadata": {
        "id": "MidlWs9jQkyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "seems like prior to leaving do a new report on the cleaned data as a test? or write tests"
      ],
      "metadata": {
        "id": "18U11sz1UP94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE FEATURES \n",
        "\n"
      ],
      "metadata": {
        "id": "Ddzi2p9CkKhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering\n",
        "\n",
        "Training / Sample "
      ],
      "metadata": {
        "id": "9fvzu_6Sx3W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SAMPLE DATA, TRAIN goal to generalize to new data well\n",
        "# randomly divide dataset into training (train and optimize )and test (to evalute)\n",
        "\n",
        "# - pull out 30% \n",
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#cross validation in orddr to help assure the final run on the test will be the best it can be \n",
        "\n",
        "\n",
        "#not working NameError: name 'model_selection' is not defined\n",
        "\n",
        "\n",
        "# Impute missing values for age on training set \n",
        "# clean training set now ?"
      ],
      "metadata": {
        "id": "0SoApj0QQKEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model \n",
        "# Confusion Matrix \n"
      ],
      "metadata": {
        "id": "_iDLoM2rT0w2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}