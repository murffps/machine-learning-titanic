{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "90CLU2dda0dp"
      ],
      "authorship_tag": "ABX9TyMLpvmuXQLD9GP2XEbPat8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murffps/machine-learning-titanic/blob/main/MLBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Project Introduction ########\n",
        "Titanic Data Prediction with Classification Supervised Learning : \n",
        "\n",
        "*   Prediction label for an individual as \"survived\" or \"died\"\n",
        "\n",
        "*    take passenger/trip information and predict whether that passenger would survive:  \n",
        "\n",
        "*   Is this a good ML question for good data for ML ? Why ? Why not? initial assumptions.\n",
        "how to properly document this \n",
        "Cost Time trade offs  princples estimate\n",
        "Business value ROI \n",
        "Success how to measure given the above https://github.com/mattharrison/ml_pocket_reference/blob/master/ch03.ipynb \n"
      ],
      "metadata": {
        "id": "90CLU2dda0dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algebreic Representation of Intent to create a function \n",
        " y = f(x) to turn a feature into a label \n",
        " x is a matrix, every column in x is a feature \n",
        " the output y is a vector that contains labels(or targets) (classification uses labels or regression would use values) \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "QsmwcvVyg4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ML WORKFLOW STEPS \n",
        "Gather\n",
        "clean \n",
        "Create feautres\n",
        "normlaize \n",
        "\n",
        "sample data\n",
        "testing data\n",
        "training data\n",
        "create model \n",
        "evaluate model - possible change question\n",
        "deploy model"
      ],
      "metadata": {
        "id": "zPXKzLacf5Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model that serves as the base for a data science process. It has six sequential phases:\n",
        "\n",
        "Business understanding – What does the business need?\n",
        "Data understanding – What data do we have / need? Is it clean?\n",
        "Data preparation – How do we organize the data for modeling?\n",
        "Modeling – What modeling techniques should we apply?\n",
        "Evaluation – Which model best meets the business objectives?\n",
        "Deployment – How do stakeholders access the results?\n",
        "Published in 1999 to standardize data mining processes across industries, it has since become the most common methodology for data mining, analytics, and data science projects.\n",
        "\n",
        "Data science teams that combine a loose implementation of CRISP-DM with overarching team-based agile project management approaches will likely see the best results.\n",
        "\n",
        "SOURCES:\n",
        "https://www.datascience-pm.com/crisp-dm-2/\n",
        "\n",
        "\n",
        "https://web.archive.org/web/20220401041957/https://www.the-modeling-agency.com/crisp-dm.pdf\n",
        "\n",
        "\n",
        "https://www.oreilly.com/library/view/machine-learning-pocket/9781492047537/\n",
        "Book sections\n",
        "Introduction\n",
        "Overview of the Machine Learning Processing\n",
        "Classification Walkthrough: Titanic Dataset\n",
        "Missing Data\n",
        "Cleaning Data\n",
        "Exploring\n",
        "Preprocess Data\n",
        "Feature Selection\n",
        "Imbalanced Classes\n",
        "Classification\n",
        "Model Selection\n",
        "Metrics and Classification Evaluation\n",
        "Explaining Models\n",
        "Regression\n",
        "Metrics and Regression Evaluation\n",
        "Explaining Regression Models\n",
        "Dimensionality Reduction\n",
        "Clustering\n",
        "Pipelines\n"
      ],
      "metadata": {
        "id": "oQJReZMedTe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create requirements.txt with required python code packages\n",
        "%%writefile requirements.txt\n",
        "matplotlib\n",
        "pandas\n",
        "sklearn \n",
        "yellowbrick\n",
        "xlrd"
      ],
      "metadata": {
        "id": "17olroObDUtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9892c557-94d4-4083-be0d-74d450affa38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install \n",
        "!pip install -r requirements.txt\n",
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip "
      ],
      "metadata": {
        "id": "qTFYNUwKDXzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tools and libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import (ensemble, preprocessing,tree)\n",
        "from sklearn.metrics import (auc, confusion_matrix, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
        "from yellowbrick.classifier import (ConfusionMatrix, ROCAUC)\n",
        "from yellowbrick.model_selection import (LearningCurve)\n",
        "import ydata_profiling\n",
        "from ydata_profiling import ProfileReport\n",
        "import IPython"
      ],
      "metadata": {
        "id": "PoNAT-O_ay0Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GATHER DATA - load an excel file\n",
        "url = (\"https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls\")\n",
        "df = pd.read_excel(url) # create a data frame (matrix of data) with column labels (as opposed to numpy array)\n",
        "orig_df = df "
      ],
      "metadata": {
        "id": "UA-rVo0YjAUp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPARE AND CLEAN DATA \n",
        "# 1. strategy to determine data readiness and quality for use\n",
        "# 2. hanlde the  issues found, once quality or readiness is determined CLEAN DATA "
      ],
      "metadata": {
        "id": "p7ZhmNLQu8Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inspect #rows columns & summary stats\n",
        "df.shape \n",
        "df.describe().iloc[:, :2]\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "ys7CE3bGyY-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFo1kYBAnjFw"
      },
      "source": [
        "\n",
        "#note on axis 0 - along the index - apply the operation on said axis \n",
        "\n",
        "\n",
        "# a. analyze or spot check data for format we can use  ie min max, \n",
        "df.isnull().sum() # count of missing data in each column \n",
        "df.isnull().sum(axis=1).loc[:10] #\n",
        "df.isnull().mean() * 100 # get the percentage of null values\n",
        "# c. check for type of values like integer or float, NA or missing data, outliers, leaky features (info that are not available for new data yet or cheating by giving data to the model)\n",
        "# d. use count for checking for missing data - only includes values not NaN - SME can help to know what to do \n",
        "# e. box plot or histogram outliers  \n",
        "\n",
        "# extended note on data types : look for pandas friendly data types to store data--> int64 float64 datetime64[ns] object (combination of string and other). \n",
        "# pandas will try to coerce data and fall back to object, convert date and strings to numeric or feature engineer one. (cardinality h/l and dummy columns?)\n",
        "mask = df.isnull().any(axis=1)\n",
        "mask.head() \n",
        "df[mask].body.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f. profile data \n",
        "ydata_profiling.ProfileReport(df)\n",
        "profile = ProfileReport(df, title=\"data set\", html={'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"file.html\")\n",
        "\n",
        "# View HTML profile report\n",
        "IPython.display.HTML(filename='/content/file.html')"
      ],
      "metadata": {
        "id": "gOSUF3c5yWW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# a. impute \n",
        "# b. to repace or deal with missing or bad data: Standardize, Normalize, Outliers(check min/max), Missing Data(check for NaN), \n",
        "# c. Use S the most common to  \n",
        "# d. drop them -- One possible strategy is to drop columns with no variance or signal or perfect or very high positive or negative correlation\n",
        "# e. use pandas to create dummy columns with all 0s \n",
        "\n",
        "\n",
        "# conclusions for dropping: \n",
        "# age (interpolate values), \n",
        "# cabin\n",
        "# boat\n",
        "# body missing data in many rows and leaks data (gives survived or deceased)\n",
        "df = df.drop(\n",
        "    columns=[\n",
        "        \"name\",\n",
        "        \"ticket\",\n",
        "        \"home.dest\",\n",
        "        \"boat\",\n",
        "        \"body\",  \n",
        "        \"cabin\"\n",
        "    ]) \n",
        "\n",
        "df.sex.value_counts(dropna=False)\n",
        "df.embarked.value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "aYurXgSMCalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE FEATURES \n",
        "name = df.name\n",
        "name.head(3)\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df)\n",
        "df.columns\n",
        "\n",
        "df = df.drop(columns=\"sex_male\")\n",
        "# OR df = pd.get_dummies(df, drop_first=True)\n",
        "df.columns\n",
        "\n",
        "y = df.survived\n",
        "X = df.drop(columns=\"survived\")"
      ],
      "metadata": {
        "id": "Ddzi2p9CkKhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SAMPLE DATA - pull out 30% \n",
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#not working NameError: name 'model_selection' is not defined\n",
        "\n",
        "# Impute missing values for age on training set "
      ],
      "metadata": {
        "id": "0SoApj0QQKEN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}