{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "90CLU2dda0dp"
      ],
      "authorship_tag": "ABX9TyNtpucEjaPm4fLw+eBBn6rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murffps/machine-learning-titanic/blob/main/MLBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Project Introduction ########\n",
        "Titanic Data Prediction with Classification Supervised Learning : \n",
        "\n",
        "*   Prediction label for an individual as \"survived\" or \"died\"\n",
        "\n",
        "*    take passenger/trip information and predict whether that passenger would survive:  \n",
        "\n",
        "*   Is this a good ML question for good data for ML ? Why ? Why not? initial assumptions.\n",
        "how to properly document this \n",
        "Cost Time trade offs  princples estimate\n",
        "Business value ROI \n",
        "Success how to measure given the above https://github.com/mattharrison/ml_pocket_reference/blob/master/ch03.ipynb \n",
        "\n",
        "many diff sets of steps - pick one \n"
      ],
      "metadata": {
        "id": "90CLU2dda0dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algebreic Representation of Intent to create a function \n",
        " y = f(x) to turn a feature into a label \n",
        " x is a matrix, every column in x is a feature \n",
        " the output y is a vector that contains labels(or targets) (classification uses labels or regression would use values) \n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "QsmwcvVyg4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ML WORKFLOW STEPS \n",
        "Gather\n",
        "clean \n",
        "Create feautres\n",
        "normlaize \n",
        "\n",
        "sample data\n",
        "testing data\n",
        "training data\n",
        "create model \n",
        "evaluate model - possible change question\n",
        "deploy model"
      ],
      "metadata": {
        "id": "zPXKzLacf5Jb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a process model that serves as the base for a data science process. It has six sequential phases:\n",
        "\n",
        "Business understanding – What does the business need?\n",
        "Data understanding – What data do we have / need? Is it clean?\n",
        "Data preparation – How do we organize the data for modeling?\n",
        "Modeling – What modeling techniques should we apply?\n",
        "Evaluation – Which model best meets the business objectives?\n",
        "Deployment – How do stakeholders access the results?\n",
        "Published in 1999 to standardize data mining processes across industries, it has since become the most common methodology for data mining, analytics, and data science projects.\n",
        "\n",
        "Data science teams that combine a loose implementation of CRISP-DM with overarching team-based agile project management approaches will likely see the best results.\n",
        "\n",
        "SOURCES:\n",
        "https://www.datascience-pm.com/crisp-dm-2/\n",
        "\n",
        "\n",
        "https://web.archive.org/web/20220401041957/https://www.the-modeling-agency.com/crisp-dm.pdf\n",
        "\n",
        "\n",
        "https://www.oreilly.com/library/view/machine-learning-pocket/9781492047537/\n",
        "Book sections\n",
        "Introduction\n",
        "Overview of the Machine Learning Processing\n",
        "Classification Walkthrough: Titanic Dataset\n",
        "Missing Data\n",
        "Cleaning Data\n",
        "Exploring\n",
        "Preprocess Data\n",
        "Feature Selection\n",
        "Imbalanced Classes\n",
        "Classification\n",
        "Model Selection\n",
        "Metrics and Classification Evaluation\n",
        "Explaining Models\n",
        "Regression\n",
        "Metrics and Regression Evaluation\n",
        "Explaining Regression Models\n",
        "Dimensionality Reduction\n",
        "Clustering\n",
        "Pipelines\n"
      ],
      "metadata": {
        "id": "oQJReZMedTe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. Business Understanding\n",
        "\n"
      ],
      "metadata": {
        "id": "O77lvAAYV2F4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TDSP (Team Data Science Process): 2 steps also taken \n",
        "\n",
        "  Data Acquisition & Understanding (Knowledge Information Patterns)\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  Afetr evaulatio Customer Acceptance\n",
        "\n"
      ],
      "metadata": {
        "id": "ojOQophuZGAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create requirements.txt with required python code packages\n",
        "%%writefile requirements.txt\n",
        "matplotlib\n",
        "pandas\n",
        "sklearn \n",
        "yellowbrick"
      ],
      "metadata": {
        "id": "17olroObDUtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856f0002-0ee2-4665-a53d-1d900cd25167"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install \n",
        "!pip install -r requirements.txt\n",
        "!pip install gwpy --quiet #supposed to supress logs"
      ],
      "metadata": {
        "id": "qTFYNUwKDXzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a539574-3342-4389-debe-ae84e0f36652"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (1.5.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post4.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (4.39.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->-r requirements.txt (line 1)) (5.12.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->-r requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick->-r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from yellowbrick->-r requirements.txt (line 4)) (1.10.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 1)) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->yellowbrick->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->yellowbrick->-r requirements.txt (line 4)) (3.1.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post4-py3-none-any.whl size=2973 sha256=1869c8af66cc15e6adcf39af26bfb9851e3b7e8ead0aa6394000c534a032a189\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/b2/a9/590d15767d34955f20a9a033e8db973b79cb5672d95790c0a9\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post4\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tools and libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import (ensemble, preprocessing,tree)\n",
        "from sklearn.metrics import (auc, confusion_matrix, roc_auc_score, roc_curve)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
        "from yellowbrick.classifier import (ConfusionMatrix, ROCAUC)\n",
        "from yellowbrick.model_selection import (LearningCurve)\n",
        "# nice way to use profiling in colab notebooks \n",
        "import sys \n",
        "!{sys.executable} -m pip install -U ydata-profiling[notebook]\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "# https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/installation.html\n",
        "from ydata_profiling import ProfileReport\n",
        "import IPython"
      ],
      "metadata": {
        "id": "PoNAT-O_ay0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680ecdbe-b8e2-4bc2-d6ff-e9ce17f65881"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ydata-profiling[notebook]\n",
            "  Downloading ydata_profiling-4.1.2-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.9/345.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (1.5.3)\n",
            "Collecting scipy<1.10,>=1.4.1\n",
            "  Downloading scipy-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting phik<0.13,>=0.11.1\n",
            "  Downloading phik-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.8/679.8 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting visions[type_image_path]==0.7.5\n",
            "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (1.22.4)\n",
            "Requirement already satisfied: pydantic<1.11,>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (1.10.7)\n",
            "Collecting htmlmin==0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm<4.65,>=4.48.2\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels<0.14,>=0.13.2 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (0.13.5)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (6.0)\n",
            "Collecting imagehash==4.3.1\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn<0.13,>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (0.12.2)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (3.1.2)\n",
            "Requirement already satisfied: requests<2.29,>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (2.27.1)\n",
            "Collecting typeguard<2.14,>=2.13.2\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Collecting multimethod<1.10,>=1.4\n",
            "  Downloading multimethod-1.9.1-py3-none-any.whl (10 kB)\n",
            "Collecting matplotlib<3.7,>=3.2\n",
            "  Downloading matplotlib-3.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (7.7.1)\n",
            "Requirement already satisfied: jupyter-client>=5.3.4 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core>=4.6.3 in /usr/local/lib/python3.9/dist-packages (from ydata-profiling[notebook]) (5.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from imagehash==4.3.1->ydata-profiling[notebook]) (8.4.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash==4.3.1->ydata-profiling[notebook]) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling[notebook]) (3.1)\n",
            "Collecting tangled-up-in-unicode>=0.0.4\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.5->ydata-profiling[notebook]) (23.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (5.5.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.0.7)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (7.34.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.6.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (5.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling[notebook]) (2.1.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=5.3.4->ydata-profiling[notebook]) (6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=5.3.4->ydata-profiling[notebook]) (2.8.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=5.3.4->ydata-profiling[notebook]) (23.2.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.6.3->ydata-profiling[notebook]) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (4.39.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (23.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<3.7,>=3.2->ydata-profiling[notebook]) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling[notebook]) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.9/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling[notebook]) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic<1.11,>=1.8.1->ydata-profiling[notebook]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling[notebook]) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling[notebook]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling[notebook]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<2.29,>=2.24.0->ydata-profiling[notebook]) (3.4)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling[notebook]) (0.5.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.0)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (67.7.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.0.38)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling[notebook]) (1.16.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (6.4.8)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.8.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.16.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.17.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (6.5.4)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (21.3.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (5.8.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (1.5.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.6)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (21.2.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (1.5.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.11.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.9.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (1.2.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (6.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.4)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (2.16.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (2.21)\n",
            "Building wheels for collected packages: htmlmin\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27096 sha256=31560a930dc5ef6cfe51ae299d7f67425638a0dc83ff31bac10caa1ced26f147\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/05/04/c6d7d3b66539d9e659ac6dfe81e2d0fd4c1a8316cc5a403300\n",
            "Successfully built htmlmin\n",
            "Installing collected packages: htmlmin, typeguard, tqdm, tangled-up-in-unicode, scipy, multimethod, jedi, matplotlib, imagehash, visions, phik, ydata-profiling\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed htmlmin-0.1.12 imagehash-4.3.1 jedi-0.18.2 matplotlib-3.6.3 multimethod-1.9.1 phik-0.12.3 scipy-1.9.3 tangled-up-in-unicode-0.2.0 tqdm-4.64.1 typeguard-2.13.3 visions-0.7.5 ydata-profiling-4.1.2\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep"
      ],
      "metadata": {
        "id": "ciCNZzyFFEAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GATHER DATA - load an excel file\n",
        "url = (\"https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.xls\")\n",
        "# create a data frame (matrix of data) with column labels \n",
        "# (as opposed to numpy array)\n",
        "df = pd.read_excel(url) \n",
        "orig_df = df "
      ],
      "metadata": {
        "id": "UA-rVo0YjAUp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PREPARE AND CLEAN DATA \n",
        "# 1. strategy to determine data readiness and quality for use\n",
        "# 2. hanlde the  issues found, once quality or readiness is determined CLEAN DATA \n",
        "\n",
        "#### Definitions and Notes and Guide on above:\n",
        "##### extended note on data types : look for pandas friendly data types to store data--> int64 float64 datetime64[ns] object (combination of string and other). \n",
        "##### pandas will try to coerce data and fall back to object, convert date and strings to numeric or feature engineer one. (cardinality h/l and dummy columns?)\n",
        "##### standardizing: some models perform better if they are standardized by given a mean value of 0 and a standard deviation of 1\n",
        "##### leaky features (info that are not available for new data yet or cheating by giving data to the model)\n",
        "##### use count for checking for missing data - only includes values not NaN - \n",
        "\n",
        "---\n",
        "\n",
        "SME can help to know what to do in case of ..\n",
        "\n",
        "---\n",
        "Data \n",
        "There are two primary types of numerical data: discrete and continuous data.\n",
        "- continuous refers to data that can be measured.\n",
        "- discrete also referred to as discrete values, is data that only takes certain values. Commonly in the form of whole numbers or integers, this is data that can be counted and has a finite number of values. These values must be able to fall within certain classifications and are unable to be broken down into smaller parts.\n",
        "median vs mode vs mean \n",
        "many exploratory methods and analytical approaches only work with specific data types. For this reason, being able to determine the nature of your data will make handling your data more manageable and effective when it comes to yielding timely insights."
      ],
      "metadata": {
        "id": "rn1BjAGUDU_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect #rows columns \n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys7CE3bGyY-r",
        "outputId": "bb2b8c90-60d3-44b0-ab97-60c37b474add"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1309, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# snapshot & summary stats\n",
        "df.describe().iloc[:, :2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "r697DM3E3bL-",
        "outputId": "20c0374e-67f2-491c-aec5-7e8cfc788ab7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            pclass     survived\n",
              "count  1309.000000  1309.000000\n",
              "mean      2.294882     0.381971\n",
              "std       0.837836     0.486055\n",
              "min       1.000000     0.000000\n",
              "25%       2.000000     0.000000\n",
              "50%       3.000000     0.000000\n",
              "75%       3.000000     1.000000\n",
              "max       3.000000     1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63bf086f-e455-4e9f-9851-3c73cc2f9403\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1309.000000</td>\n",
              "      <td>1309.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.294882</td>\n",
              "      <td>0.381971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.837836</td>\n",
              "      <td>0.486055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63bf086f-e455-4e9f-9851-3c73cc2f9403')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-63bf086f-e455-4e9f-9851-3c73cc2f9403 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-63bf086f-e455-4e9f-9851-3c73cc2f9403');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for type of values we mostly want integer or float for now\n",
        "df.dtypes\n",
        "\n",
        "# conclusion, therefor what\n",
        "# why are all these objects? what does that mean? how to deal with?\n",
        "# name          object\n",
        "# sex           object\n",
        "# ticket        object\n",
        "# cabin         object\n",
        "# embarked      object\n",
        "# boat          object\n",
        "# home.dest     object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMzfB06u3TfW",
        "outputId": "690fbbea-f68d-4612-ec55-76d82d83de2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pclass         int64\n",
              "survived       int64\n",
              "name          object\n",
              "sex           object\n",
              "age          float64\n",
              "sibsp          int64\n",
              "parch          int64\n",
              "ticket        object\n",
              "fare         float64\n",
              "cabin         object\n",
              "embarked      object\n",
              "boat          object\n",
              "body         float64\n",
              "home.dest     object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFo1kYBAnjFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f15a406-ddab-4716-8694-131514d008a1"
      },
      "source": [
        "# analyze or spot check data for proper format  \n",
        "df.isnull().sum() # one way to get a count of missing data in each column "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pclass          0\n",
              "survived        0\n",
              "name            0\n",
              "sex             0\n",
              "age           263\n",
              "sibsp           0\n",
              "parch           0\n",
              "ticket          0\n",
              "fare            1\n",
              "cabin        1014\n",
              "embarked        2\n",
              "boat          823\n",
              "body         1188\n",
              "home.dest     564\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# by default apply axis 0 means along the index - percentage of null values \n",
        "# axis one if you want each sample or along columns \n",
        "df.isnull().sum(axis=1).loc[:10] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GkeoaDA3rLt",
        "outputId": "b8ab3f43-b91f-406d-acd6-c60b44c8f0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1\n",
              "1     1\n",
              "2     2\n",
              "3     1\n",
              "4     2\n",
              "5     1\n",
              "6     1\n",
              "7     2\n",
              "8     1\n",
              "9     2\n",
              "10    1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the percentage of null values \n",
        "# TIP python treats True and False as 1 and 0 so this shows % of missing data\n",
        "df.isnull().mean() * 100 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJJ0D_c_3u1W",
        "outputId": "785f8665-a49e-454e-d1ac-10bfa9f10bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pclass        0.000000\n",
              "survived      0.000000\n",
              "name          0.000000\n",
              "sex           0.000000\n",
              "age          20.091673\n",
              "sibsp         0.000000\n",
              "parch         0.000000\n",
              "ticket        0.000000\n",
              "fare          0.076394\n",
              "cabin        77.463713\n",
              "embarked      0.152788\n",
              "boat         62.872422\n",
              "body         90.756303\n",
              "home.dest    43.086325\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  inspect at rows with missing data by using a Boolean array \n",
        "mask = df.isnull().any(axis=1)\n",
        "mask.head()  # rows\n",
        "# data.columns = data.columns.str.strip() handle white space \n",
        "#conclusions: \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85w_Lms95HTd",
        "outputId": "4a020358-52f7-4221-deaa-ff85f19e7383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    True\n",
              "1    True\n",
              "2    True\n",
              "3    True\n",
              "4    True\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[mask].body.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiS01GGt5DM6",
        "outputId": "0a471b0c-9eec-423c-a0db-de1ae3660bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      NaN\n",
              "1      NaN\n",
              "2      NaN\n",
              "3    135.0\n",
              "4      NaN\n",
              "Name: body, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# f. profile data \n",
        "profile = ProfileReport(df, title=\"titanic data set\", html={'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"file.html\")\n",
        "# View HTML profile report\n",
        "IPython.display.HTML(filename='/content/file.html')"
      ],
      "metadata": {
        "id": "gOSUF3c5yWW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# name = df.name\n",
        "# name.head(3)\n",
        "# ? more inspecting priper to drop this one?"
      ],
      "metadata": {
        "id": "6Qw244s6RSSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further inspect data with advanced box plot or histogram to spot outliers wit seaborn (matplotlib ++ or something) \n",
        "#  what needs to meet outside of the profile report ? \n",
        "# Findings from Profile Report ✨ finding issues vs trends suspicions... \n",
        "# ready to move on to cleaning ? when ...\n",
        "#, NA or missing data, outliers, leaky features "
      ],
      "metadata": {
        "id": "6vbDVhS9C0Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the \"spread\" of the data considering:\n",
        "#  variance - the average of the squared differences from the \n",
        "  #mean SYMBOL here\n",
        "# standard deviation  \n",
        "# covariance \n",
        "# correlation\n",
        "\n",
        "# Handle Outliers(check min/max, ) seems like the most common sense while in the graphs plots before the code ones \n",
        "\n",
        "# https://www.udemy.com/course/data-science-and-machine-learning-with-python-hands-on/learn/lecture/4020118?start=315#overview"
      ],
      "metadata": {
        "id": "SrNNo0yyTTFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLEAN (vs PREPARE? is prepare after import or import a certain way ) STEPS or even OR Data Wrangling \n",
        "# Normalize (\"preprocess\")\n",
        "# Standardize - translate the data so that it has a mean value of 0 and a standard deviation of 1 (helps for scaling)\n"
      ],
      "metadata": {
        "id": "Y3rVBxMBTTyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle Missing Data\n",
        "# Examine - profile gives good intel or we could use missingno\n",
        "# (check for NaN, null) (a-e options)\n",
        "\n",
        "# age has 263 (20.1%) missing values - strategy to handle  \n",
        "# cabin has 1014 (77.5%) missing values\t- strategy to handle  \n",
        "# boat has 823 (62.9%) missing values\t- strategy to handle  - drop why impute why - S why .. goal - dummy columns \n",
        "# body has 1188 (90.8%) missing values - strategy to handle  \n",
        "# home.dest has 564 (43.1%) missing values - strategy to handle  "
      ],
      "metadata": {
        "id": "-oJ2lqCETQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a. impute -  refers to the procedure of using alternative values in place of missing data. Like an estimate or other instead of deletion.\n",
        "# b. to replace deal with missing or bad data: 0's is bad data\n",
        "# sibsp has 891 (68.1%) zeros\t\n",
        "# parch has 1002 (76.5%) zeros\t\n",
        "# fare has 17 (1.3%) zeros\t"
      ],
      "metadata": {
        "id": "aYurXgSMCalL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_iDLoM2rT0w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d. use pandas to create dummy columns with all 0s \n",
        "df = pd.get_dummies(df)\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDiXz-5RQZyp",
        "outputId": "5e9935ce-1212-4bc9-f825-503a4d9f834a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['pclass', 'survived', 'age', 'sibsp', 'parch', 'fare', 'body',\n",
              "       'name_Abbing, Mr. Anthony', 'name_Abbott, Master. Eugene Joseph',\n",
              "       'name_Abbott, Mr. Rossmore Edward',\n",
              "       ...\n",
              "       'home.dest_Wimbledon Park, London / Hayling Island, Hants',\n",
              "       'home.dest_Windsor, England New York, NY', 'home.dest_Winnipeg, MB',\n",
              "       'home.dest_Winnipeg, MN', 'home.dest_Woodford County, KY',\n",
              "       'home.dest_Worcester, England', 'home.dest_Worcester, MA',\n",
              "       'home.dest_Yoevil, England / Cottage Grove, OR',\n",
              "       'home.dest_Youngstown, OH', 'home.dest_Zurich, Switzerland'],\n",
              "      dtype='object', length=2841)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e. drop them -- One possible strategy is to drop columns with \n",
        "     # reasons for dropping: no variance or signal or perfect or very high positive or negative correlation\n",
        "     # # c. Use S the most common to  \n",
        "     \n",
        "# age (interpolate values), \n",
        "# cabin\n",
        "# boat - dtype not supported issues \n",
        "# body missing data in many rows and leaks data (gives survived or deceased)\n",
        "df = df.drop(\n",
        "    columns=[\n",
        "        \"name\",\n",
        "        \"ticket\",\n",
        "        \"home.dest\",\n",
        "        \"boat\",\n",
        "        \"body\",  \n",
        "        \"cabin\" \n",
        "    ]) \n",
        "\n",
        "df.sex.value_counts(dropna=False)\n",
        "df.embarked.value_counts(dropna=False)\n",
        "df = df.drop(columns=\"sex_male\")\n",
        "# OR df = pd.get_dummies(df, drop_first=True)\n",
        "df.columns\n",
        "y = df.survived\n",
        "X = df.drop(columns=\"survived\")"
      ],
      "metadata": {
        "id": "MidlWs9jQkyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "seems like prior to leaving do a new report on the cleaned data as a test? or write tests"
      ],
      "metadata": {
        "id": "18U11sz1UP94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CREATE FEATURES \n",
        "\n"
      ],
      "metadata": {
        "id": "Ddzi2p9CkKhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering\n",
        "\n",
        "Training / Sample "
      ],
      "metadata": {
        "id": "9fvzu_6Sx3W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SAMPLE DATA, TRAIN \n",
        "# - pull out 30% \n",
        "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "#not working NameError: name 'model_selection' is not defined\n",
        "\n",
        "# Impute missing values for age on training set \n",
        "# clean training set now ?"
      ],
      "metadata": {
        "id": "0SoApj0QQKEN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}